{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# You can add your own functions here according to your decision tree implementation.\n",
    "# There is no restriction on following the below template, these fucntions are here to simply help you.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import xlogy\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to perform one hot encoding on the input data\n",
    "    \"\"\"\n",
    "    # Initialize OneHotEncoder\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "    # Fit and transform the data\n",
    "    encoded_array = encoder.fit_transform(X)\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(X.columns))\n",
    "    return encoded_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(y):\n",
    "    '''\n",
    "    Function to calculate variance, avoiding nan.\n",
    "    y: variable to calculate variance. Should be a Pandas Series.\n",
    "    '''\n",
    "    if len(y) == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return y.var()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ifreal(y: pd.Series) -> bool:\n",
    "    \"\"\"\n",
    "    Function to check if the given series has real or discrete values\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if(pd.api.types.is_bool_dtype(y)):\n",
    "        return False\n",
    "    elif (pd.api.types.is_any_real_numeric_dtype(y)):\n",
    "        unique_ration = y.nunique()/y.len()\n",
    "        if(unique_ration<0.05):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    elif (pd.api.types.is_object_dtype(y)):\n",
    "        unique_ration = y.nunique()/y.len()\n",
    "        if(unique_ration<0.05):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else :\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(Y: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the entropy\n",
    "    \"\"\"\n",
    "    if (check_ifreal(Y)==False):\n",
    "        prob = Y.unique()\n",
    "        val_cnts = Y.value_counts()\n",
    "        # tot_cnt = Y.value_counts().sum()\n",
    "        entropy = 0.0\n",
    "        for y in val_cnts:\n",
    "            proportion = y/len(Y)\n",
    "            # entropy+= proportion*np.log2(proportion)\n",
    "            entropy+=xlogy(proportion,proportion)/np.log(2)\n",
    "        entropy=-entropy\n",
    "        return entropy   \n",
    "    else:\n",
    "        mean = Y.mean()\n",
    "        mse = ((Y- mean) ** 2).mean()\n",
    "        return mse\n",
    "        \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(Y: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the gini index\n",
    "    \"\"\"\n",
    "    if(check_ifreal(Y)==False):  # non numeric values\n",
    "        pob = Y.unique()\n",
    "        val_cnts = Y.value_counts()\n",
    "        gi=0\n",
    "     \n",
    "        for y in val_cnts:\n",
    "            proportion = (y/len(Y))**2\n",
    "            gi = gi + proportion\n",
    "        return 1-gi\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(Y: pd.Series, attr: pd.Series, criterion: str) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the information gain using criterion (entropy, gini index or MSE)\n",
    "    \"\"\"\n",
    "\n",
    "    # Real output ==> Use MSE/Variance\n",
    "    # Case 1: Real Input, Real Output\n",
    "    if check_ifreal(attr) and check_ifreal(Y):\n",
    "        parent_variance = np.var(Y)\n",
    "        left_variance = np.var(Y[attr <= attr.mean()])\n",
    "        right_variance = np.var(Y[attr > attr.mean()])\n",
    "        weights = [len(Y[attr <= attr.mean()]) / len(Y), len(Y[attr > attr.mean()]) / len(Y)]\n",
    "        weighted_variance = weights[0] * left_variance + weights[1] * right_variance\n",
    "        return parent_variance - weighted_variance\n",
    "\n",
    "    # Case 2: Discrete Input, Real Output\n",
    "    elif not check_ifreal(attr) and check_ifreal(Y):\n",
    "        parent_variance = np.var(Y)\n",
    "        uniq_attr = np.unique(attr)\n",
    "        weighted_variances = 0\n",
    "        for attribute in uniq_attr:\n",
    "            Y_filtered = Y[attr == attribute]\n",
    "            weight = len(Y_filtered) / len(Y)\n",
    "            weighted_variances += weight * np.var(Y_filtered)\n",
    "        return parent_variance - weighted_variances\n",
    "\n",
    "    # Discrete output ==> Use Entropy or Gini Index\n",
    "    # Case 3: Real Input, Discrete Output\n",
    "    elif check_ifreal(attr) and not check_ifreal(Y):\n",
    "        parent_impurity = entropy(Y) if criterion == \"information_gain\" else gini_index(Y)\n",
    "        threshold = attr.mean()\n",
    "        values = [attr <= threshold, attr > threshold]  # Discretize the feature\n",
    "        weights = [len(Y[attr <= threshold]) / len(Y), len(Y[attr > threshold]) / len(Y)]\n",
    "        weighted_impurities = 0\n",
    "        for i in range(2):\n",
    "            child_impurity = entropy(Y[values[i]]) if criterion == \"information_gain\" else gini_index(Y[values[i]])\n",
    "            weighted_impurities += weights[i] * child_impurity\n",
    "        return parent_impurity - weighted_impurities\n",
    "    \n",
    "    # Case 4: Discrete Input, Discrete Output\n",
    "    else:\n",
    "        parent_impurity = entropy(Y) if criterion == \"information_gain\" else gini_index(Y)\n",
    "        uniq_attr = np.unique(attr)\n",
    "        weighted_impurities = 0\n",
    "        for attribute in uniq_attr:\n",
    "            Y_filtered = Y[attr == attribute]\n",
    "            weight = len(Y_filtered) / len(Y)\n",
    "            child_impurity = entropy(Y_filtered) if criterion == \"information_gain\" else gini_index(Y_filtered)\n",
    "            weighted_impurities += weight * child_impurity\n",
    "        return parent_impurity - weighted_impurities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_split_attribute(X: pd.DataFrame, y: pd.Series, criterion, features: pd.Series):\n",
    "    \"\"\"\n",
    "    Function to find the optimal attribute to split about.\n",
    "    If needed you can split this function into 2, one for discrete and one for real valued features.\n",
    "    You can also change the parameters of this function according to your implementation.\n",
    "\n",
    "    features: pd.Series is a list of all the attributes we have to split upon\n",
    "\n",
    "    return: attribute to split upon\n",
    "    \"\"\"\n",
    "\n",
    "    # According to wheather the features are real or discrete valued and the criterion, find the attribute from the features series with the maximum information gain (entropy or varinace based on the type of output) or minimum gini index (discrete output).\n",
    "\n",
    "    best_feature = None\n",
    "    best_gain = -float('inf')\n",
    "\n",
    "    # Loop through all features to calculate their information gain\n",
    "    for feature in features:\n",
    "        gain = information_gain(y, X[feature], criterion)\n",
    "\n",
    "        # Update best feature if current one has higher gain\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feature = feature\n",
    "\n",
    "    return best_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X: pd.DataFrame, y: pd.Series, attribute, value):\n",
    "    \"\"\"\n",
    "    Funtion to split the data according to an attribute.\n",
    "    If needed you can split this function into 2, one for discrete and one for real valued features.\n",
    "    You can also change the parameters of this function according to your implementation.\n",
    "\n",
    "    attribute: attribute/feature to split upon\n",
    "    value: value of that attribute to split upon\n",
    "\n",
    "    return: splitted data(Input and output)\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the data based on a particular value of a particular attribute. You may use masking as a tool to split the data.\n",
    "\n",
    "    if check_ifreal(X[attribute]):\n",
    "        # For real-valued features, split based on the threshold value\n",
    "        X_left = X[X[attribute] <= value]\n",
    "        y_left = y[X[attribute] <= value]\n",
    "        X_right = X[X[attribute] > value]\n",
    "        y_right = y[X[attribute] > value]\n",
    "    else:\n",
    "        # For discrete features, split based on equality with the given value\n",
    "        X_left = X[X[attribute] == value]\n",
    "        y_left = y[X[attribute] == value]\n",
    "        X_right = X[X[attribute] != value]\n",
    "        y_right = y[X[attribute] != value]\n",
    "\n",
    "    return (X_left, y_left), (X_right, y_right)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
